{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337adb6b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0709bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc13b44",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec3b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 600 rows → ../data/subsets/train_subset_0.01.csv\n",
      "Saved 200 rows → ../data/subsets/val_subset_0.01.csv\n"
     ]
    }
   ],
   "source": [
    "# TODO: use the ./image folder \n",
    "DATA_FOLDER = \n",
    "SUBSET_DIR = f\"../data/subsets\"\n",
    "os.makedirs(SUBSET_DIR, exist_ok=True)\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "train_csv = f\"{DATA_FOLDER}/train.csv\"\n",
    "val_csv   = f\"{DATA_FOLDER}/val.csv\"\n",
    "\n",
    "# fraction - percentage of the data used\n",
    "fraction = 0.01\n",
    "\n",
    "# frac - percentage of the data used\n",
    "def sample_csv(src_csv, dst_csv, frac=fraction, seed=42):\n",
    "    df = pd.read_csv(src_csv)\n",
    "    df_sample = df.sample(frac=frac, random_state=seed).reset_index(drop=True)\n",
    "    df_sample.to_csv(dst_csv, index=False)\n",
    "    print(f\"Saved {len(df_sample)} rows → {dst_csv}\")\n",
    "\n",
    "sample_csv(train_csv, f\"{SUBSET_DIR}/train_subset_{fraction}.csv\")\n",
    "sample_csv(val_csv, f\"{SUBSET_DIR}/val_subset_{fraction}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b3d81b",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d87988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {\n",
    "    'VenusExpress': 1, 'Cheops': 2, 'LisaPathfinder': 3, 'ObservationSat1': 4,\n",
    "    'Proba2': 5, 'Proba3': 6, 'Proba3ocs': 7, 'Smart1': 8, 'Soho': 9, 'XMM Newton': 10\n",
    "}\n",
    "\n",
    "class PyTorchSPARKDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_path, class_map, root_dir=DATA_FOLDER, split=\"train\"):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.class_map = class_map\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        sat = row['Class']\n",
    "        img_name = row['Image name']\n",
    "        mask_name = row['Mask name']\n",
    "        bbox = literal_eval(row['Bounding box'])\n",
    "\n",
    "        # Load image\n",
    "        img_path = f\"{self.root_dir}/images/{sat}/{self.split}/{img_name}\"\n",
    "        pil_image = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = pil_image.size\n",
    "        image = torch.ByteTensor(torch.ByteStorage.from_buffer(pil_image.tobytes()))\n",
    "        image = image.view(h, w, 3).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = f\"{self.root_dir}/mask/{sat}/{self.split}/{mask_name}\"\n",
    "        pil_mask = Image.open(mask_path).convert(\"L\")\n",
    "        w2, h2 = pil_mask.size\n",
    "        mask = torch.ByteTensor(torch.ByteStorage.from_buffer(pil_mask.tobytes()))\n",
    "        mask = mask.view(h2, w2)[None].float() / 255.0\n",
    "\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        boxes = torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)\n",
    "        labels = torch.tensor([self.class_map[sat]], dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": mask,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": torch.tensor([(x2 - x1)*(y2 - y1)], dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor([0])\n",
    "        }\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbfb8b",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ab2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes=11):\n",
    "    model = models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # Update classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Update mask head\n",
    "    in_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_mask, 256, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc6cf4",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8e9f222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 200)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = PyTorchSPARKDataset(f\"{SUBSET_DIR}/train_subset_{fraction}.csv\", class_map, split=\"train\")\n",
    "val_ds   = PyTorchSPARKDataset(f\"{SUBSET_DIR}/val_subset_{fraction}.csv\", class_map, split=\"val\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ed5ed",
   "metadata": {},
   "source": [
    "# Debug  Single Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "186e5e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/702j9k1958l02w4hq777w5f40000gn/T/ipykernel_13557/3017965666.py:27: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  image = torch.ByteTensor(torch.ByteStorage.from_buffer(pil_image.tobytes()))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4.218687534332275,\n",
       " {'loss_classifier': tensor(2.4092, grad_fn=<NllLossBackward0>),\n",
       "  'loss_box_reg': tensor(0.1432, grad_fn=<DivBackward0>),\n",
       "  'loss_mask': tensor(1.6195, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  'loss_objectness': tensor(0.0436, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>),\n",
       "  'loss_rpn_box_reg': tensor(0.0031, grad_fn=<DivBackward0>)})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = create_model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# run 1 batch\n",
    "images, targets = next(iter(train_loader))\n",
    "images = [img.to(device) for img in images]\n",
    "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "model.train()\n",
    "loss_dict = model(images, targets)\n",
    "loss = sum(loss_dict.values())\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "loss.item(), loss_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d420cbf",
   "metadata": {},
   "source": [
    "# Small Batch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a4f6715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 12/150 [01:24<16:13,  7.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m loss = \u001b[38;5;28msum\u001b[39m(loss_dict.values())\n\u001b[32m     18\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m optimizer.step()\n\u001b[32m     22\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversityofLuxembourg/Uni Lu/Sem3 Classes/Computer Vision and Image Analysis/project/computer-vision-zeta-25/.venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversityofLuxembourg/Uni Lu/Sem3 Classes/Computer Vision and Image Analysis/project/computer-vision-zeta-25/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/CloudStorage/OneDrive-UniversityofLuxembourg/Uni Lu/Sem3 Classes/Computer Vision and Image Analysis/project/computer-vision-zeta-25/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = create_model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, targets in tqdm(train_loader):\n",
    "        images = [i.to(device) for i in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch} Train Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Save state dict\n",
    "torch.save(model.state_dict(), \"final_state_dict.pth\")\n",
    "print(\"Saved → final_state_dict.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
