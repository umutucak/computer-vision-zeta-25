{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import helper class\n",
    "from utils import SPARKDataset, PyTorchSPARKDataset\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_dir = \"/Users/umut/Documents/computer-vision-zeta-25/data/\"\n",
    "split = \"val\"\n",
    "\n",
    "class_map= {'VenusExpress':0, 'Cheops':1, 'LisaPathfinder':2, 'ObservationSat1':3, 'Proba2':4, 'Proba3':5,\n",
    "                           'Proba3ocs' :6, 'Smart1':7, 'Soho':8, 'XMM Newton':9} # Class map\n",
    "\n",
    "dataset = SPARKDataset(class_map, root_dir=dataset_root_dir,split=split) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 4\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        dataset.visualize(randint(0, 6000),size = (10,10),ax=axes[i][j])\n",
    "        axes[i][j].axis('off')\n",
    "fig.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "cols = 4\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 15))\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        dataset.visualize(randint(0, 6000),size = (10,10),ax=axes[i][j],mask_visualize=True)\n",
    "        axes[i][j].axis('off')\n",
    "fig.tight_layout() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PyTorchSPARKDataset(class_map, root_dir=dataset_root_dir,split=\"train\")\n",
    "val_dataset = PyTorchSPARKDataset(class_map, root_dir=dataset_root_dir,split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0  # Index of the sample you want to retrieve\n",
    "image, sample = val_dataset[sample_idx]\n",
    "\n",
    "# Now you can access the image, mask, bbox, and class from the sample\n",
    "mask = sample['masks']\n",
    "bbox = sample['boxes']\n",
    "class_label = sample['class']\n",
    "\n",
    "# If you want to display the image, you can use matplotlib, but remember to convert it back to a PIL image or a NumPy array\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Convert the tensor image to a NumPy array and display it\n",
    "# Note: PyTorch tensors are in CxHxW format and need to be converted to HxWxC for matplotlib\n",
    "reverse_class_map = {v: k for k, v in class_map.items()}\n",
    "\n",
    "# Use the class label to get the corresponding class name\n",
    "class_name = reverse_class_map[class_label.item()]\n",
    "\n",
    "\n",
    "image_np = image.numpy().transpose((1, 2, 0))\n",
    "plt.imshow(image_np)\n",
    "plt.title(f'Class: {class_name}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, collate_fn=collate_fn, shuffle=True, drop_last=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=2, collate_fn=collate_fn, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increment for background\n",
    "num_classes = len(class_map) + 1\n",
    "\n",
    "# pretrained mask r-cnn\n",
    "model_det = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# fix amount of classes (coco pretrained model has 80 outputs)\n",
    "in_features = model_det.roi_heads.box_predictor.cls_score.in_features\n",
    "model_det.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "in_features_mask = model_det.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model_det.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, 256, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_det.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_det.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_det.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, samples in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True):\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        samples = [{k: v.to(device) for k, v in t.items()} for t in samples]\n",
    "\n",
    "        loss_dict = model_det(imgs, samples)\n",
    "        loss = sum(loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_det.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, targets in val_loader:\n",
    "        imgs = [img.to(device) for img in imgs]\n",
    "        predictions = model_det(imgs)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "img = imgs[0].permute(1,2,0).cpu().numpy()\n",
    "mask = predictions[0][\"masks\"][0,0].cpu().numpy()\n",
    "box = predictions[0][\"boxes\"][0].cpu().numpy()\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.imshow(mask, alpha=0.5)\n",
    "x1,y1,x2,y2 = box\n",
    "plt.gca().add_patch(plt.Rectangle((x1,y1), x2-x1, y2-y1,\n",
    "                                  fill=False, edgecolor='red'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2\n",
    "our dataset has colorcoded segments embedded in images, so ill crop out the bounding boxes and use a pre-implemented u-net model.\n",
    "\n",
    "In `utils.py` we have:\n",
    "- hashmap for mapping colors to labels\n",
    "- utility function for converting the mask colors from the data to labels\n",
    "- new dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import PART_COLOR_MAP\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch.nn as nn\n",
    "\n",
    "num_classes = len(PART_COLOR_MAP) + 1 # new class amount (2)\n",
    "\n",
    "model_unet = smp.Unet(\n",
    "    encoder_name=\"resnet34\", # architecture\n",
    "    encoder_weights=\"imagenet\", # pretrained weights\n",
    "    in_channels=3,\n",
    "    classes=num_classes\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function: CrossEntropy because after some research i found that it is the standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model_unet.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model_unet.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True):\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model_unet(imgs)\n",
    "        loss = criterion(preds, masks)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: loss: {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
